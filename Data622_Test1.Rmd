---
title: "Test 1"
author: "Erinda Budo"
date: "11/15/2020"
output:
  html_document:
    theme: united
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This test is an extension of HW1.Will add Bagging and Loocv for KNN3,KNN5 and NB 

## Load Packages
```{r, message=FALSE, warning=FALSE}
library(caret)
library(e1071)
library(class)
library(ROCR)
library(ggplot2)
library(dplyr)
library(tidyr)
library(kableExtra)
library(C50)
options(knitr.kable.NA = '')
```


## Import  data

```{r}
raw_data<-read.csv("https://raw.githubusercontent.com/ErindaB/Data-622/master/HW1.csv",stringsAsFactors = TRUE)
```

## EDA


```{r}
head(raw_data) %>% kable() %>% kable_styling() %>% scroll_box(width = "800px", height = "400px")
```


```{r}
summary(raw_data)
```

## Data Preparation


```{r}
#  normalize X variable  for optimal performance for KNN
norm <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}
raw_data <- raw_data
raw_data$X <- norm(raw_data$X)

#Create a dummy variable for Y variable
raw_data <- raw_data %>%
  mutate(Y_a = ifelse(Y == "a", 1, 0),
         Y_b = ifelse(Y == "b", 1, 0),
         Y_c = ifelse(Y == "c", 1, 0),
         Y_d = ifelse(Y == "d", 1, 0),
         Y_e = ifelse(Y == "e", 1, 0)) %>%
  select(-Y)
#the data needs to be divided into training and test sets so we can evaluate the ability of the algorithms
set.seed(300)
indxTrain <- createDataPartition(y = raw_data$label,p = 0.8,list = FALSE)
train_data <- raw_data[indxTrain,]
test_data  <- raw_data[-indxTrain,]
# These subsets will help with training and evaluation
train_data_no_label <- train_data %>% select(-label)
test_df_no_label  <- test_data %>% select(-label)
train_data_label <- train_data$label
test_data_label <- test_data$label
test_data_no_label  <- test_data %>% select(-label)
# create metrics for evaluation
algo <- function(ground_truth, yhat, algo){
  cm <- confusionMatrix(table(ground_truth, yhat))
  cm_table <- cm$table
  tpr <- cm_table[[1]] / (cm_table[[1]] + cm_table[[4]])
  fnr <- 1 - tpr
  fpr <- cm_table[[3]] / (cm_table[[3]] + cm_table[[4]])
  tnr <- 1 - fpr
  accuracy <- cm$overall[[1]]
  for_auc <- prediction(c(yhat), ground_truth)
  auc <- performance(for_auc, "auc")
  auc <- auc@y.values[[1]]
  return(data.frame(Algo = algo, AUC = auc, ACCURACY = accuracy, TPR = tpr, FPR = fpr, TNR = tnr, FNR = fnr))
}

```


## Baseline Models 

### Ability to Learn.


### Logistic Regression

```{r}
#returns predictions for the  data.frame
LR_func <- function(lr_model, data, label_col_name, threshold = 0.5){
  data_levels <- levels(data[[label_col_name]])
  cols_to_keep <- label_col_name != names(data)
  data <- data[,cols_to_keep]
  lr_yhat <- predict(lr_model, data, type = "response")
  lr_yhat <- as.factor(ifelse(lr_yhat <= threshold, data_levels[1], data_levels[2]))
  return(lr_yhat)
}

LR <- glm(label ~ ., data = train_data, family = "binomial")
LR_train <- LR_func(LR, train_data, "label")
ability_to_learn <- algo(train_data_label, LR_train, "LR")
```




### Naive Bayes

```{r}
NB <- naiveBayes(label ~ ., data = train_data)
train_nb <- predict(NB, train_data_no_label)
ability_to_learn <- rbind(ability_to_learn, algo(train_data_label, train_nb, "NB"))
```

### KNN

```{r}
knn3 <- knn(train_data_no_label, train_data_no_label, train_data_label, k = 3)
ability_to_learn <- rbind(ability_to_learn, algo(train_data_label, knn3, "KNN3"))
knn5 <- knn(train_data_no_label, train_data_no_label, train_data_label, k = 5)
ability_to_learn <- rbind(ability_to_learn, algo(train_data_label, knn5, "KNN5"))


```



### Ability to Generalize


### Logistic Regression

```{r}
LR_G <- LR_func(LR, test_data, "label")
ability_to_generalize <-algo(test_data_label, LR_G, "LR")
```

### Naive Bayes

```{r}
NB_G <- predict(NB, test_data_no_label)
ability_to_generalize <- rbind(ability_to_generalize, algo(test_data_label, NB_G, "NB"))
```

### KNN

```{r}
KNN3_G <- knn(train_data_no_label, test_data_no_label, train_data_label, k = 3)
ability_to_generalize <- rbind(ability_to_generalize, algo(test_data_label, KNN3_G, "KNN3"))
KNN5_G <- knn(train_data_no_label, test_data_no_label, train_data_label, k = 5)
ability_to_generalize <- rbind(ability_to_generalize, algo(test_data_label, KNN5_G, "KNN5"))
```




##  Bagging

I will try bagging KNN models,NB.I will use the models trained on the bootstrap samples to predict the test data and collect metrics on the ability to learn and generalize, which will be consolidated.

```{r}
get_bootstrap_sample <- function(data, bootstrap_proportion = 0.6, sample_with_replacement = TRUE){
  n_bootstrap_observations <- round(nrow(data) * bootstrap_proportion, 0)
  return(data[sample(nrow(data), n_bootstrap_observations, replace = sample_with_replacement),])
}

n_bags <- 100
the_bags <- list()
for (i in 1:n_bags){
  bag_df <- get_bootstrap_sample(train_data)
  bag_df_without_label <- bag_df %>% select(-label)
  bag_df_label <- bag_df$label
  the_bags[[i]] <- bag_df
  
  # NB Model
  bag_nb_model <- naiveBayes(label ~ ., data = bag_df)
  training_bag_nb_yhat <- predict(bag_nb_model, bag_df_without_label)
  ## Ability to learn
  bag_capacity_to_learn <- algo(bag_df_label, training_bag_nb_yhat, paste("NB Bag", i))
  if(exists("nb_bag_capacity_to_learn")){
    nb_bag_capacity_to_learn <- rbind(nb_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    nb_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Ability to generalize
  bag_nb_test_yhat <- predict(bag_nb_model, test_df_no_label)
  bag_capacity_to_generalize <- algo(test_data_label, bag_nb_test_yhat, paste("NB Bag", i))
  if(exists("nb_bag_capacity_to_generalize")){
    nb_bag_capacity_to_generalize <- rbind(nb_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    nb_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
  #  KNN3 Model
  ## Ability to learn
  training_bag_knn3_yhat <- knn(bag_df_without_label, bag_df_without_label, bag_df_label, k = 3)
  bag_capacity_to_learn <- algo(bag_df_label, training_bag_knn3_yhat, paste("KNN3 Bag", i))
  if(exists("knn3_bag_capacity_to_learn")){
    knn3_bag_capacity_to_learn <- rbind(knn3_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    knn3_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Ability to generalize
  bag_knn3_test_yhat <- knn(bag_df_without_label, test_df_no_label, bag_df_label, k = 3)
  bag_capacity_to_generalize <- algo(test_data_label, bag_knn3_test_yhat, paste("KNN3_Bagging", i))
  if(exists("knn3_bag_capacity_to_generalize")){
    knn3_bag_capacity_to_generalize <- rbind(knn3_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    knn3_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
  # KNN5 Model
  ## Ability to learn
  training_bag_knn5_yhat <- knn(bag_df_without_label, bag_df_without_label, bag_df_label, k = 5)
  bag_capacity_to_learn <- algo(bag_df_label, training_bag_knn5_yhat, paste("KNN5_Bagging", i))
  if(exists("knn5_bag_capacity_to_learn")){
    knn5_bag_capacity_to_learn <- rbind(knn5_bag_capacity_to_learn, bag_capacity_to_learn)
  } else {
    knn5_bag_capacity_to_learn <- bag_capacity_to_learn
  }
  ## Ability to generalize
  bag_knn5_test_yhat <- knn(bag_df_without_label, test_df_no_label, bag_df_label, k = 5)
  bag_capacity_to_generalize <- algo(test_data_label, bag_knn5_test_yhat, paste("KNN5_Bagging", i))
  if(exists("knn5_bag_capacity_to_generalize")){
    knn5_bag_capacity_to_generalize <- rbind(knn5_bag_capacity_to_generalize, bag_capacity_to_generalize)
  } else {
    knn5_bag_capacity_to_generalize <- bag_capacity_to_generalize
  }
  
}
```


```{r}

average_metrics <- function(metrics, algo){
  avg_metrics <- metrics %>%
  select(-Algo) %>%
  colMeans(na.rm = TRUE) %>%
  t() %>%
  data.frame("Algo" = c(algo), .)
  return(avg_metrics)
}

# KNN3
ability_to_learn <- rbind(ability_to_learn, average_metrics(knn3_bag_capacity_to_learn, "KNN3_Bagging"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(knn3_bag_capacity_to_generalize, "KNN3_Bagging"))
# KNN5
ability_to_learn <- rbind(ability_to_learn, average_metrics(knn5_bag_capacity_to_learn, "KNN5_Bagging"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(knn5_bag_capacity_to_generalize, "KNN5_ Bagging"))
# NB
ability_to_learn <- rbind(ability_to_learn, average_metrics(nb_bag_capacity_to_learn, "NB_Bagging"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(nb_bag_capacity_to_generalize, "NB_Bagging"))

```

##  LOOCV


```{r}
training_loocv_nb_yhat <- c()
training_loocv_knn3_yhat <- c()
training_loocv_knn5_yhat <- c()
loocv_test_label <- c()
loocv_dfs <- list()

for (i in 1:nrow(train_data)){
  # Leave One Out
  loocv_test <- train_data[i,]
  loocv_test_without_label <- loocv_test %>% select(-label)
  loocv_test_label <- c(loocv_test_label, loocv_test$label)
  loocv_training_df <- train_data[-c(i),]
  loocv_training_df_without_label <- loocv_training_df %>% select(-label)
  loocv_training_df_label <- loocv_training_df$label
  loocv_dfs[[i]] <- loocv_training_df
  
  # Naive Bayes Model
  loocv_nb_model <- naiveBayes(label ~ ., data = loocv_training_df)
  training_loocv_nb_yhat <- c(training_loocv_nb_yhat, predict(loocv_nb_model, loocv_test_without_label))
  loocv_nb_test_yhat <- predict(loocv_nb_model, test_df_no_label)
  loocv_capacity_to_generalize <- algo(test_data_label, loocv_nb_test_yhat, paste("NB with LOOCV", i))
  if(exists("nb_loocv_capacity_to_generalize")){
    nb_loocv_capacity_to_generalize <- rbind(nb_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    nb_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }
  
 ## the KNN3 Model
  
  training_loocv_knn3_yhat <- c(training_loocv_knn3_yhat, knn(loocv_training_df_without_label, loocv_test_without_label, loocv_training_df_label, k = 3))
 
  loocv_knn3_test_yhat <- knn(loocv_training_df_without_label, test_df_no_label, loocv_training_df_label, k = 3)
  loocv_capacity_to_generalize <- algo(test_data_label, loocv_knn3_test_yhat, paste("KNN3 with LOOCV", i))
  if(exists("knn3_loocv_capacity_to_generalize")){
    knn3_loocv_capacity_to_generalize <- rbind(knn3_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    knn3_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }
  
  #  KNN5 Model

  training_loocv_knn5_yhat <- c(training_loocv_knn5_yhat, knn(loocv_training_df_without_label, loocv_test_without_label, loocv_training_df_label, k = 5))
  loocv_knn5_test_yhat <- knn(loocv_training_df_without_label, test_df_no_label, loocv_training_df_label, k = 5)
  loocv_capacity_to_generalize <- algo(test_data_label, loocv_knn5_test_yhat, paste("KNN5 with LOOCV", i))
  if(exists("knn5_loocv_capacity_to_generalize")){
    knn5_loocv_capacity_to_generalize <- rbind(knn5_loocv_capacity_to_generalize, loocv_capacity_to_generalize)
  } else {
    knn5_loocv_capacity_to_generalize <- loocv_capacity_to_generalize
  }

}


# KNN3
ability_to_learn <- rbind(ability_to_learn, algo(loocv_test_label, training_loocv_knn3_yhat, "KNN3_LOOCV"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(knn3_loocv_capacity_to_generalize, "KNN3_LOOCV"))
# KNN5
ability_to_learn <- rbind(ability_to_learn, algo(loocv_test_label, training_loocv_knn5_yhat, "KNN5_LOOCV"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(knn5_loocv_capacity_to_generalize, "KNN5 with LOOCV"))
# NB
ability_to_learn <- rbind(ability_to_learn, algo(loocv_test_label, training_loocv_nb_yhat, "NB_LOOCV"))
ability_to_generalize <- rbind(ability_to_generalize, average_metrics(nb_loocv_capacity_to_generalize, "NB_LOOCV"))

```



## Summary 


**Table 1. Ability to Learn**
```{r, echo=FALSE}
ability_to_learn %>% arrange(Algo) %>% kable() %>%  kable_styling()
```

**Table 2. Ability to Generalize**
```{r, echo=FALSE}
ability_to_generalize %>% arrange(Algo) %>% kable() %>% kable_styling()
```

## Conclusion

Bagging model and kNN model with k=3  has the best performance in training data, it has the best ability to learn among all other models.

Interestingly,LOOCV performs better for KNN5 and KNN3 in the ability to generalize,but performs worse for NB .
Bagging and Loocv do not do better than simpler models in both training data and testing data.Simpler models have the best performance for both training and testing data.



